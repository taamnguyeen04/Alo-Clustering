
# üìö Student Clustering Project

## üöÄ Gi·ªõi thi·ªáu

D·ª± √°n n√†y th·ª±c hi·ªán **clustering h·ªçc sinh** d·ª±a tr√™n ƒëi·ªÉm s·ªë, h·∫°nh ki·ªÉm, danh hi·ªáu, gi√°o vi√™n ch·ªß nhi·ªám v√† m·ªôt s·ªë ch·ªâ s·ªë kh√°c.  
M·ª•c ti√™u l√† **t·ª± ƒë·ªông nh√≥m h·ªçc sinh** th√†nh c√°c c·ª•m c√≥ √Ω nghƒ©a m√† kh√¥ng c·∫ßn nh√£n, s·ª≠ d·ª•ng k·ªπ thu·∫≠t **representation learning** k·∫øt h·ª£p **self-supervised learning** v√† **unsupervised clustering**.

---

## üõ†Ô∏è Pipeline Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu

**File:** [`dataset.py`](./dataset.py)

- ƒê·ªçc d·ªØ li·ªáu t·ª´ file `.xlsx`.
- C√°c lo·∫°i c·ªôt:
  - **Numerical**: ƒëi·ªÉm s·ªë c√°c m√¥n h·ªçc, x·∫øp h·∫°ng, t·ªïng k·∫øt,...
  - **Ordinal**: h·ªçc l·ª±c, h·∫°nh ki·ªÉm, danh hi·ªáu (c√≥ th·ª© t·ª±).
  - **Nominal**: gi√°o vi√™n ch·ªß nhi·ªám (kh√¥ng th·ª© t·ª±).
- **Ti·ªÅn x·ª≠ l√Ω:**
  - Numerical: Impute b·∫±ng **median**, chu·∫©n h√≥a **StandardScaler**.
  - Ordinal: Impute b·∫±ng **most frequent**, encode b·∫±ng **OrdinalEncoder** theo mapping.
  - Nominal: Impute b·∫±ng **most frequent**, encode b·∫±ng **OrdinalEncoder**.

- D·ªØ li·ªáu sau x·ª≠ l√Ω ƒë∆∞·ª£c chuy·ªÉn th√†nh **TensorDataset**.

---

## üèóÔ∏è C·∫•u Tr√∫c M√¥ H√¨nh

**File:** [`model.py`](./model.py)

### 1. Encoder
- √Ånh x·∫° ƒë·∫ßu v√†o th√†nh vector nh√∫ng (`embedding`).
- C·∫•u tr√∫c: 3 t·∫ßng Linear + BatchNorm + ReLU + Dropout.

### 2. Clustering Head
- D·ª± ƒëo√°n ph√¢n c·ª•m t·ª´ embedding.
- C·∫•u tr√∫c: Linear -> ReLU -> Dropout -> Linear (ra logits c·ª•m).

---

## üß† Ph√¢n T√≠ch C√°c H√†m Loss

**File:** [`train.py`](./train.py)

H√†m `total_loss_fn` = t·ªï h·ª£p nhi·ªÅu th√†nh ph·∫ßn:

| Th√†nh ph·∫ßn | C√¥ng th·ª©c | Gi·∫£i th√≠ch |
| :--- | :--- | :--- |
| **Contrastive Loss** | $$ \mathcal{L}_{\text{contrastive}} = \text{CrossEntropy}(\text{sim}(z_1, z_2)/T) $$ | Th√∫c ƒë·∫©y augmentations c·ªßa c√πng m·∫´u g·∫ßn nhau. |
| **Clustering Loss** | $$ \mathcal{L}_{\text{cluster}} = \text{KL}(p \parallel q) $$ | L√†m ph√¢n ph·ªëi d·ª± ƒëo√°n s·∫Øc n√©t v√† c√¢n b·∫±ng. |
| **Compactness Loss** | $$ \mathcal{L}_{\text{compact}} = \frac{1}{N} \sum_{k=1}^{K} \sum_{i \in C_k} \| z_i - \mu_k \|^2 $$ | C√°c ƒëi·ªÉm trong c·ª•m g·∫ßn t√¢m c·ª•m. |
| **Separation Loss** | $$ \mathcal{L}_{\text{separation}} = \sum_{i \neq j} \frac{1}{\| \mu_i - \mu_j \|^2 + \epsilon} $$ | T√¢m c√°c c·ª•m c√†ng xa nhau c√†ng t·ªët. |

Trong ƒë√≥:
- \( z_1, z_2 \) l√† c√°c embedding sau augmentation.
- \( T \) l√† temperature.
- \( p \) l√† ph√¢n ph·ªëi softmax output.
- \( q \) l√† ph√¢n ph·ªëi sharpened.
- \( \mu_k \) l√† t√¢m c·ª•m th·ª© \(k\).
- \( C_k \) l√† t·∫≠p ƒëi·ªÉm thu·ªôc c·ª•m \(k\).

### T·ªïng Loss:

$$
\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{contrastive}} + \beta \mathcal{L}_{\text{cluster}} + \gamma \mathcal{L}_{\text{compact}} + \delta \mathcal{L}_{\text{separation}}
$$

```mermaid
graph LR;
    X[Input Features] --> Encoder --> Z[Embedding] --> ClusteringHead --> ClusterLogits;
```
---

## üî• Qu√° Tr√¨nh Hu·∫•n Luy·ªán

- Augmentation ƒë·∫ßu v√†o: noise, dropout, scale jitter.
- Hu·∫•n luy·ªán encoder + clustering head c√πng l√∫c.
- Early stopping d·ª±a tr√™n Silhouette Score.
- L∆∞u checkpoint t·ªët nh·∫•t.
- TensorBoard ƒë·ªÉ theo d√µi loss v√† metrics.
- Visualization c·ª•m h·ªçc sinh theo t·ª´ng epoch.

### Flowchart t·ªïng quan:

```mermaid
flowchart TD
    A[Input Data] --> B[Preprocessing]
    B --> C[Encoder]
    C --> D[Embedding Space]
    D --> E[Clustering Head]
    E --> F[Cluster Assignment]
    D --> G[Loss Calculation]
    F --> G
    G --> H[Backpropagation]
```

---

## üìä Ph∆∞∆°ng Ph√°p ƒê√°nh Gi√° Clustering

Khi th·ª±c hi·ªán ph√¢n c·ª•m (clustering), ta c·∫ßn c√°c ch·ªâ s·ªë ƒë·ªÉ **ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng** c·ªßa k·∫øt qu·∫£. Ba ph∆∞∆°ng ph√°p ph·ªï bi·∫øn nh·∫•t g·ªìm:

---

### 1. **Silhouette Score**

- **Gi·ªõi thi·ªáu:**  
Silhouette Score ƒëo l∆∞·ªùng **m·ª©c ƒë·ªô g·∫Øn k·∫øt** (cohesion) c·ªßa c√°c ƒëi·ªÉm trong c√πng c·ª•m v√† **m·ª©c ƒë·ªô ph√¢n t√°ch** (separation) gi·ªØa c√°c c·ª•m kh√°c nhau.

- **C√¥ng th·ª©c:**  
ƒê·ªëi v·ªõi m·ªói ƒëi·ªÉm \( i \):
$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$

Trong ƒë√≥:
- \( a(i) \) = kho·∫£ng c√°ch trung b√¨nh t·ª´ ƒëi·ªÉm \( i \) ƒë·∫øn t·∫•t c·∫£ c√°c ƒëi·ªÉm kh√°c trong c√πng c·ª•m.
- \( b(i) \) = kho·∫£ng c√°ch trung b√¨nh t·ª´ ƒëi·ªÉm \( i \) ƒë·∫øn t·∫•t c·∫£ c√°c ƒëi·ªÉm ·ªü c·ª•m g·∫ßn nh·∫•t kh√°c c·ª•m \(i\).

Gi√° tr·ªã t·ªïng th·ªÉ l√† trung b√¨nh \( s(i) \) tr√™n t·∫•t c·∫£ c√°c ƒëi·ªÉm.

- **√ù nghƒ©a:**  
  - \( s(i) \) g·∫ßn **1** ‚Üí ƒëi·ªÉm n·∫±m **g·ªçn** trong c·ª•m c·ªßa n√≥.
  - \( s(i) \) g·∫ßn **0** ‚Üí ƒëi·ªÉm n·∫±m **gi·ªØa** hai c·ª•m.
  - \( s(i) \) g·∫ßn **-1** ‚Üí ƒëi·ªÉm c√≥ th·ªÉ b·ªã **g√°n nh·∫ßm c·ª•m**.

- **M·ª•c ti√™u:** C√†ng cao c√†ng t·ªët.

---

### 2. **Calinski-Harabasz Score** (CH Score)

- **Gi·ªõi thi·ªáu:**  
CH Score (c√≤n g·ªçi l√† **Variance Ratio Criterion**) ƒëo t·ª∑ l·ªá gi·ªØa **ph√¢n t√°n gi·ªØa c√°c c·ª•m** v√† **ph√¢n t√°n b√™n trong c·ª•m**.

- **C√¥ng th·ª©c:**  
Gi·∫£ s·ª≠ c√≥ \( k \) c·ª•m v√† t·ªïng \( N \) ƒëi·ªÉm:
$$
\text{CH} = \frac{\text{tr}(B_k)}{\text{tr}(W_k)} \times \frac{N - k}{k - 1}
$$

Trong ƒë√≥:
- \( \text{tr}(B_k) \) = trace (t·ªïng ƒë∆∞·ªùng ch√©o) c·ªßa ma tr·∫≠n ph√¢n t√°n **gi·ªØa c√°c c·ª•m**.
- \( \text{tr}(W_k) \) = trace c·ªßa ma tr·∫≠n ph√¢n t√°n **trong c·ª•m**.

- **√ù nghƒ©a:**  
  - CH Score c√†ng l·ªõn ‚Üí c√°c c·ª•m c√†ng t√°ch bi·ªát r√µ v√† c√†ng ch·∫∑t ch·∫Ω b√™n trong.
  - Ph√π h·ª£p cho c√°c t·∫≠p d·ªØ li·ªáu c√≥ h√¨nh d·∫°ng c·ª•m ƒë∆°n gi·∫£n (convex).

- **M·ª•c ti√™u:** C√†ng cao c√†ng t·ªët.

---

### 3. **Davies-Bouldin Score** (DB Score)

- **Gi·ªõi thi·ªáu:**  
DB Score ƒëo l∆∞·ªùng **m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng** gi·ªØa c√°c c·ª•m ‚Äî c·ª•m c√†ng t∆∞∆°ng t·ª± nhau c√†ng **t·ªá**.

- **C√¥ng th·ª©c:**  
$$
\text{DB} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left( \frac{S_i + S_j}{M_{ij}} \right)
$$

Trong ƒë√≥:
- \( S_i \) = ƒë·ªô r·ªông (dispersion) trung b√¨nh c·ªßa c·ª•m \( i \).
- \( M_{ij} \) = kho·∫£ng c√°ch gi·ªØa t√¢m c·ª•m \( i \) v√† \( j \).

- **√ù nghƒ©a:**  
  - Ch·ªâ s·ªë th·∫•p ‚Üí c√°c c·ª•m nh·ªè g·ªçn v√† xa nhau ‚Üí clustering t·ªët.
  - Ch·ªâ s·ªë cao ‚Üí c√°c c·ª•m l·ªõn, ch·ªìng l·∫•n ‚Üí clustering t·ªá.

- **M·ª•c ti√™u:** C√†ng th·∫•p c√†ng t·ªët.

---

## ‚ú® T√≥m t·∫Øt nhanh

| Ch·ªâ s·ªë | C√¥ng th·ª©c t·ªïng quan                                                | M·ª•c ti√™u |  
| :--- |:-------------------------------------------------------------------| :--- |
| **Silhouette Score** | $$ \frac{b(i) - a(i)}{\max(a(i), b(i)} $$                          | C√†ng cao c√†ng t·ªët |
| **Calinski-Harabasz Score** | $$ \frac{\text{tr}(B_k)}{\text{tr}(W_k)} \times \frac{N-k}{k-1} $$ | C√†ng cao c√†ng t·ªët |
| **Davies-Bouldin Score** | $$Trung b√¨nh max ( \frac{S_i + S_j}{M_{ij}} )$$                    | C√†ng th·∫•p c√†ng t·ªët |



---

## ‚öôÔ∏è H∆∞·ªõng D·∫´n Ch·∫°y

```bash
# C√†i th∆∞ vi·ªán c·∫ßn thi·∫øt
pip install -r requirements.txt

# Hu·∫•n luy·ªán m√¥ h√¨nh
python train.py
```

Outputs:
- Logs: `out/logs/`
- M√¥ h√¨nh: `out/best_model.pt`
- Bi·ªÉu ƒë·ªì c·ª•m: `out/clustering_epoch_X.png`

---

## üìä K·∫øt Qu·∫£ So S√°nh
### ƒê√°nh gi√° tr√™n t·∫≠p median_imputed.xlsx
| M√¥ h√¨nh                      | Silhouette ‚Üë | Calinski-Harabasz ‚Üë | Davies-Bouldin ‚Üì |
|------------------------------| ------------ | ------------------- | ---------------- |
| **Alo**                      | 0.8671       | 121540.13           | 0.1604           |
| **KMeans**                   | 0.5790       | 88582.15            | 0.5760           |
| **Expectation-Maximization** | 0.5772       | 88175.58            | 0.5760           |
| **Birch**                    | 0.5355       | 56833.10            | 0.5036           |
| **DBSCAN**                   | -0.2036      | 13719.50            | 2.9958           |

### ƒê√°nh gi√° tr√™n t·∫≠p group_imputed.xlsx
| M√¥ h√¨nh                      | Silhouette ‚Üë | Calinski-Harabasz ‚Üë | Davies-Bouldin ‚Üì |
|------------------------------| ------------ |---------------------|------------------|
| **Alo**                      | 0.8567       | 110529.42           | 0.1803           |
| **KMeans**                   | 0.5804       | 88881.94            | 0.5662           |
| **Expectation-Maximization** | 0.5799       | 88014.62            | 0.5611           |
| **Birch**                    | 0.5721       | 85146.38            | 0.5798           |
| **DBSCAN**                   | -0.1154      | 18351.34            | 2.8008           |

### ƒê√°nh gi√° tr√™n t·∫≠p knn_imputed.xlsx
| M√¥ h√¨nh                      | Silhouette ‚Üë | Calinski-Harabasz ‚Üë | Davies-Bouldin ‚Üì |
|------------------------------| ------------ |--------------------| ---------------- |
| **Alo**                      | 0.8528       | 98327.59           | 0.1702           |
| **KMeans**                   | 0.5846       | 90206.86           | 0.5662           |
| **Expectation-Maximization** | 0.5837       | 89988.08           | 0.5663           |
| **Birch**                    | 0.5318       | 69103.21           | 0.5988           |
| **DBSCAN**                   | -0.0852      | 20293.55           | 2.7151           |

### ƒê√°nh gi√° tr√™n t·∫≠p linear_imputed.xlsx
| M√¥ h√¨nh                      | Silhouette ‚Üë | Calinski-Harabasz ‚Üë | Davies-Bouldin ‚Üì |
|------------------------------|--------------|---------------------| ---------------- |
| **Alo**                      | 0.8762       | 137777.17           | 0.1555           |
| **KMeans**                   | 0.5820       | 89615.72            | 0.5713           |
| **Expectation-Maximization** | 0.5801       | 89203.88            | 0.5713           |
| **Birch**                    | 0.5394       | 72300.08            | 0.5981           |
| **DBSCAN**                   | -0.2147      | 14706.45            | 2.7132           |

### ƒê√°nh gi√° tr√™n t·∫≠p mode_imputed.xlsx
| M√¥ h√¨nh                      | Silhouette ‚Üë | Calinski-Harabasz ‚Üë | Davies-Bouldin ‚Üì |
|------------------------------| ------------ |---------------------| ---------------- |
| **Alo**                      | 0.8688       | 123037.59           | 0.1607           |
| **KMeans**                   | 0.5790       | 88547.13            | 0.5761           |
| **Expectation-Maximization** | 0.5769       | 88090.56            | 0.5761           |
| **Birch**                    | 0.5717       | 85060.64            | 0.5832           |
| **DBSCAN**                   | -0.2030      | 13530.97            | 2.8810           |